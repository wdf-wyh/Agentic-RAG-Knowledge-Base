# ğŸ” å·¥ç¨‹æ·±åº¦è®²è§£ - é«˜çº§å®ç°ç»†èŠ‚ä¸ä»£ç æµç¨‹

> **é¢å‘å¼€å‘è€…** | è¯¦ç»†è®²è§£æ¯ä¸ªæ¨¡å—çš„ä»£ç å®ç°ã€å…³é”®ç®—æ³•ã€è®¾è®¡æ¨¡å¼
>
> ç‰ˆæœ¬: 3.0.0 | æœ€åæ›´æ–°: 2024å¹´12æœˆ31æ—¥

---

## ç›®å½•

1. [ä»£ç ç»„ç»‡ä¸å¯¼å…¥](#ä»£ç ç»„ç»‡ä¸å¯¼å…¥)
2. [å…³é”®ç±»ä¸æ¥å£](#å…³é”®ç±»ä¸æ¥å£)
3. [ç®—æ³•è¯¦è§£](#ç®—æ³•è¯¦è§£)
4. [å¼‚å¸¸å¤„ç†ä¸æ—¥å¿—](#å¼‚å¸¸å¤„ç†ä¸æ—¥å¿—)
5. [å•å…ƒæµ‹è¯•](#å•å…ƒæµ‹è¯•)
6. [éƒ¨ç½²ä¸é…ç½®](#éƒ¨ç½²ä¸é…ç½®)
7. [æ€§èƒ½åŸºå‡†æµ‹è¯•](#æ€§èƒ½åŸºå‡†æµ‹è¯•)

---

## ä»£ç ç»„ç»‡ä¸å¯¼å…¥

### 1. é¡¹ç›®ç›®å½•ç»“æ„è¯¦è§£

```
RAGçŸ¥è¯†åº“/
â”‚
â”œâ”€â”€ src/                           # æºä»£ç æ ¹ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent/                    # ğŸ†• Agent æ™ºèƒ½ä½“æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py           # å¯¼å‡º: RAGAgent, AgentBuilder, AgentConfig
â”‚   â”‚   â”œâ”€â”€ base.py               # åŸºç±»ä¸æ•°æ®ç»“æ„ (738 è¡Œ)
â”‚   â”‚   â”œâ”€â”€ rag_agent.py          # RAG Agent å®ç°
â”‚   â”‚   â”œâ”€â”€ intent_router.py      # æ„å›¾è·¯ç”±å™¨
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ base.py           # å·¥å…·åŸºç±» (Tool ABC)
â”‚   â”‚       â”œâ”€â”€ rag_tools.py      # RAG æ£€ç´¢å·¥å…·
â”‚   â”‚       â”œâ”€â”€ web_tools.py      # ç½‘é¡µæœç´¢å·¥å…·
â”‚   â”‚       â”œâ”€â”€ file_tools.py     # æ–‡ä»¶æ“ä½œå·¥å…·
â”‚   â”‚       â””â”€â”€ analysis_tools.py # åˆ†æå·¥å…·
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                      # FastAPI åº”ç”¨å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py                # FastAPI å·¥å‚å‡½æ•° (create_app)
â”‚   â”‚   â”œâ”€â”€ routes.py             # åŸæœ‰ API è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ agent_routes.py       # ğŸ†• Agent API è·¯ç”±
â”‚   â”‚   â””â”€â”€ v1/                   # ç‰ˆæœ¬åŒ– API
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒæ•°æ®å¤„ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py # æ–‡æ¡£åŠ è½½ä¸åˆ†å— (243 è¡Œ)
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # å‘é‡å­˜å‚¨ä¸æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py     # BM25 å…³é”®è¯æ£€ç´¢
â”‚   â”‚   â””â”€â”€ reranker.py           # CrossEncoder ç²¾æ’
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                 # ä¸šåŠ¡æœåŠ¡å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag_assistant.py      # RAG é—®ç­”æœåŠ¡ (594 è¡Œ)
â”‚   â”‚   â”œâ”€â”€ conversation_manager.py  # å¯¹è¯å†å²ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ ollama_client.py      # Ollama å®¢æˆ·ç«¯
â”‚   â”‚   â””â”€â”€ deepseek_client.py    # DeepSeek å®¢æˆ·ç«¯
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # æ•°æ®æ¨¡å‹ä¸ Schema
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py            # Pydantic æ•°æ®æ¨¡å‹
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                   # å…¨å±€é…ç½®
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py           # Config å…¨å±€é…ç½®ç±»
â”‚   â”‚
â”‚   â””â”€â”€ utils/                    # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py             # æ—¥å¿—é…ç½®
â”‚       â””â”€â”€ helpers.py            # è¾…åŠ©å‡½æ•°
â”‚
â”œâ”€â”€ frontend/                     # Vue.js 3 å‰ç«¯åº”ç”¨
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.js
â”‚   â”‚   â”œâ”€â”€ App.vue               # ä¸»å®¹å™¨ç»„ä»¶ (1907 è¡Œ)
â”‚   â”‚   â””â”€â”€ styles.css            # æ ·å¼è¡¨
â”‚   â”‚
â”‚   â””â”€â”€ dist/                     # æ‰“åŒ…è¾“å‡ºç›®å½•
â”‚
â”œâ”€â”€ documents/                    # çŸ¥è¯†åº“æºæ–‡ä»¶
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tutorial.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ vector_db/                    # å‘é‡æ•°æ®åº“å­˜å‚¨
â”‚   â”œâ”€â”€ chroma.sqlite3            # ChromaDB æ•°æ®åº“æ–‡ä»¶
â”‚   â””â”€â”€ *.parquet                 # å‘é‡æ•°æ®æ–‡ä»¶
â”‚
â”œâ”€â”€ conversations/                # å¯¹è¯å†å²å­˜å‚¨
â”‚   â”œâ”€â”€ 3b733ec7-xxxx.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ logs/                         # æ—¥å¿—è¾“å‡º
â”‚   â”œâ”€â”€ backend.log
â”‚   â””â”€â”€ agent.log
â”‚
â”œâ”€â”€ app_api.py                    # FastAPI å¯åŠ¨æ–‡ä»¶ (å…¼å®¹å±‚)
â”œâ”€â”€ run_api.py                    # API æœåŠ¡å¯åŠ¨è„šæœ¬
â”œâ”€â”€ run_cli.py                    # å‘½ä»¤è¡Œæ¥å£å¯åŠ¨è„šæœ¬
â”œâ”€â”€ run_agent.py                  # Agent äº¤äº’å¼å¯åŠ¨è„šæœ¬
â”œâ”€â”€ run_web.py                    # Streamlit Web ç•Œé¢å¯åŠ¨è„šæœ¬
â”œâ”€â”€ main.py                       # ä¸»ç¨‹åºå…¥å£ (é‡å®šå‘åˆ° run_cli.py)
â”‚
â”œâ”€â”€ requirements.txt              # Python ä¾èµ–
â”œâ”€â”€ .env.example                  # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ START_HERE.md                 # å¿«é€Ÿå¼€å§‹æŒ‡å—
â””â”€â”€ start.sh                      # ä¸€é”®å¯åŠ¨è„šæœ¬
```

### 2. æ¨¡å—å¯¼å…¥å…³ç³»

```
ç”¨æˆ·è¾“å…¥
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI (app_api.py)                                â”‚
â”‚ â”œâ”€ routes.py (API è·¯ç”±)                             â”‚
â”‚ â””â”€ agent_routes.py (Agent è·¯ç”±)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ä¸šåŠ¡å±‚ Services                                      â”‚
â”‚ â”œâ”€ RAGAssistant (src/services/rag_assistant.py)    â”‚
â”‚ â”œâ”€ RAGAgent (src/agent/rag_agent.py)               â”‚
â”‚ â”œâ”€ IntentRouter (src/agent/intent_router.py)       â”‚
â”‚ â””â”€ ConversationManager (src/services/...)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ•°æ®å¤„ç†å±‚ Core                                      â”‚
â”‚ â”œâ”€ DocumentProcessor (åŠ è½½å’Œåˆ†å—)                   â”‚
â”‚ â”œâ”€ VectorStore (å‘é‡åŒ–å’Œå­˜å‚¨)                       â”‚
â”‚ â”œâ”€ BM25Retriever (å…³é”®è¯æ£€ç´¢)                       â”‚
â”‚ â””â”€ CrossEncoderReranker (ç²¾æ’)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¤–éƒ¨æœåŠ¡                                             â”‚
â”‚ â”œâ”€ ChromaDB (å‘é‡æ•°æ®åº“)                            â”‚
â”‚ â”œâ”€ OpenAI / Ollama (LLM)                            â”‚
â”‚ â”œâ”€ DuckDuckGo / Tavily (ç½‘é¡µæœç´¢)                   â”‚
â”‚ â””â”€ Sentence-Transformers (Embedding)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. å…³é”®å¯¼å…¥è¯­å¥

```python
# src/agent/rag_agent.py
from src.agent.base import BaseAgent, AgentConfig, ThoughtStep, StreamEvent
from src.agent.tools import RAGTool, WebSearchTool, FileTool
from src.services.rag_assistant import RAGAssistant
from src.config.settings import Config

# src/services/rag_assistant.py
from src.core.vector_store import VectorStore
from src.core.bm25_retriever import BM25Retriever
from langchain_classic.chains.retrieval_qa.base import RetrievalQA
from langchain.chat_models import init_chat_model

# src/core/vector_store.py
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

# src/api/app.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.api.routes import router
from src.api.agent_routes import router as agent_router
```

---

## å…³é”®ç±»ä¸æ¥å£

### 1. Agent åŸºç±»ï¼ˆReAct æ¡†æ¶æ ¸å¿ƒï¼‰

**æ–‡ä»¶**: `src/agent/base.py`

```python
@dataclass
class StreamEvent:
    """æµå¼äº‹ä»¶ - ç”¨äºå®æ—¶æ¨é€ Agent æ€è€ƒè¿‡ç¨‹"""
    type: str  # 'thinking' | 'action' | 'observation' | 'answer' | 'error' | 'done'
    data: Any = None
    step: int = 0
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸º JSON ç”¨äº SSE æ¨é€"""
        return json.dumps({
            'type': self.type,
            'data': self.data,
            'step': self.step
        })


@dataclass
class AgentConfig:
    """Agent é…ç½®å‚æ•°"""
    max_iterations: int = 5           # æœ€å¤§æ¨ç†æ­¥æ•°ï¼ˆé™åˆ¶è®¡ç®—ï¼‰
    temperature: float = 0.7          # LLM æ¸©åº¦å‚æ•°ï¼ˆåˆ›æ„åº¦ï¼‰
    enable_reflection: bool = False   # æ˜¯å¦å¯ç”¨åæ€æœºåˆ¶ï¼ˆè€—æ—¶ï¼‰
    enable_planning: bool = True      # æ˜¯å¦å¯ç”¨è§„åˆ’èƒ½åŠ›
    verbose: bool = True              # è¯¦ç»†è¾“å‡º
    llm_timeout: int = 30             # LLM è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰


@dataclass
class ThoughtStep:
    """å•ä¸ªæ¨ç†æ­¥éª¤çš„è®°å½•"""
    step: int                         # æ­¥éª¤åºå· (1, 2, 3...)
    thought: str                      # "æˆ‘éœ€è¦åšä»€ä¹ˆï¼Ÿ" - æ€è€ƒå†…å®¹
    action: Optional[str] = None      # å·¥å…·å ("rag_search", "web_search")
    action_input: Optional[Dict] = None  # å·¥å…·å‚æ•° ({"query": "..."})
    observation: Optional[str] = None    # å·¥å…·è¿”å›ç»“æœ
    observation_data: Optional[Dict] = None  # ç»“æ„åŒ–æ•°æ® (åˆ—è¡¨ã€è¡¨æ ¼ç­‰)
    reflection: Optional[str] = None  # "ç»“æœæ»¡è¶³äº†å—ï¼Ÿ" - åæ€å†…å®¹


class BaseAgent(ABC):
    """Agent åŸºç±» - å®ç° ReAct æ¨ç†å¾ªç¯"""
    
    def __init__(self, config: AgentConfig = None):
        # AgentConfig æ˜¯ä¸€ä¸ªæ•°æ®ç±»ï¼ˆdataclassï¼‰ï¼Œç”¨äºé…ç½® RAG Agent çš„è¡Œä¸ºå‚æ•°
        self.config = config or AgentConfig()
        self.llm = self._init_llm()
        self.tools = self._init_tools()
        self.logger = logging.getLogger(__name__)
    
    def run(self, query: str) -> AgentResponse:
        """æ‰§è¡Œ ReAct æ¨ç†å¾ªç¯"""
        
        thought_process: List[ThoughtStep] = []
        tools_used: List[str] = []
        
        for iteration in range(self.config.max_iterations):
            # ç¬¬1æ­¥ï¼šTHINK - æ€è€ƒ
            thought = self._generate_thought(query, thought_process)
            
            # ç¬¬2æ­¥ï¼šPLAN - è§„åˆ’
            if self.config.enable_planning:
                plan = self._generate_plan(thought)
            else:
                plan = None
            
            # ç¬¬3æ­¥ï¼šACT - è¡ŒåŠ¨
            action_text = self._parse_action(thought)
            if not action_text:
                # æ— éœ€è¿›ä¸€æ­¥è¡ŒåŠ¨ï¼Œè¿›å…¥æœ€ç»ˆç­”æ¡ˆ
                break
            
            tool_name, tool_input = self._parse_action_input(action_text)
            
            # ç¬¬4æ­¥ï¼šOBSERVE - è§‚å¯Ÿ
            observation = self._execute_tool(tool_name, tool_input)
            tools_used.append(tool_name)
            
            # è®°å½•æœ¬æ­¥éª¤
            step = ThoughtStep(
                step=iteration + 1,
                thought=thought,
                action=tool_name,
                action_input=tool_input,
                observation=observation
            )
            thought_process.append(step)
            
            # ç¬¬5æ­¥ï¼šREFLECT - åæ€ï¼ˆå¯é€‰ï¼‰
            if self.config.enable_reflection:
                reflection = self._generate_reflection(observation)
                step.reflection = reflection
            
            # åˆ¤æ–­æ˜¯å¦å·²å¾—å‡ºç»“è®º
            if self._is_final_answer(observation):
                break
        
        # ç¬¬6æ­¥ï¼šFINAL ANSWER - ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = self._generate_final_answer(
            query=query,
            thought_process=thought_process
        )
        
        return AgentResponse(
            success=True,
            answer=final_answer,
            thought_process=thought_process,
            tools_used=tools_used,
            iterations=len(thought_process)
        )
    
    def run_stream(self, query: str) -> Generator[StreamEvent, None, None]:
        """æµå¼æ‰§è¡Œ ReAct æ¨ç†å¾ªç¯ï¼ˆç”¨äºå®æ—¶æ¨é€ï¼‰"""
        
        try:
            thought_process: List[ThoughtStep] = []
            
            for iteration in range(self.config.max_iterations):
                # æ¨é€æ€è€ƒäº‹ä»¶
                thought = self._generate_thought(query, thought_process)
                yield StreamEvent(
                    type='thinking',
                    data={'thought': thought, 'step': iteration + 1}
                )
                
                # è§£æè¡ŒåŠ¨
                tool_name, tool_input = self._parse_action_input(thought)
                if not tool_name:
                    break
                
                # æ¨é€è¡ŒåŠ¨äº‹ä»¶
                yield StreamEvent(
                    type='action',
                    data={'tool': tool_name, 'input': tool_input, 'step': iteration + 1}
                )
                
                # æ‰§è¡Œå·¥å…·
                observation = self._execute_tool(tool_name, tool_input)
                
                # æ¨é€è§‚å¯Ÿäº‹ä»¶
                yield StreamEvent(
                    type='observation',
                    data={'result': observation, 'step': iteration + 1}
                )
                
                # è®°å½•æ­¥éª¤
                thought_process.append(ThoughtStep(
                    step=iteration + 1,
                    thought=thought,
                    action=tool_name,
                    action_input=tool_input,
                    observation=observation
                ))
                
                if self._is_final_answer(observation):
                    break
            
            # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_answer = self._generate_final_answer(query, thought_process)
            
            # æ¨é€ç­”æ¡ˆäº‹ä»¶
            yield StreamEvent(
                type='answer',
                data={'answer': final_answer}
            )
            
            # æ¨é€å®Œæˆäº‹ä»¶
            yield StreamEvent(type='done')
            
        except Exception as e:
            self.logger.error(f"Agent æ‰§è¡Œå‡ºé”™: {str(e)}", exc_info=True)
            yield StreamEvent(
                type='error',
                data={'message': str(e)}
            )
```

### 2. RAG åŠ©æ‰‹ç±»

**æ–‡ä»¶**: `src/services/rag_assistant.py`

```python
class RAGAssistant:
    """RAG æ£€ç´¢å¢å¼ºç”ŸæˆåŠ©æ‰‹"""
    
    def __init__(
        self,
        vector_store: VectorStore = None,
        model_name: str = None,
        temperature: float = None,
        fast_mode: bool = None
    ):
        """åˆå§‹åŒ– RAG åŠ©æ‰‹"""
        self.vector_store = vector_store or VectorStore()
        self.fast_mode = fast_mode if fast_mode is not None else Config.RAG_FAST_MODE
        
        # åˆå§‹åŒ– LLM
        self.llm = self._init_llm(model_name, temperature)
        
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.bm25_retriever = BM25Retriever()
        
        # å¯é€‰ï¼šç²¾æ’æ¨¡å‹
        self.reranker = None
        if Config.ENABLE_RERANK:
            try:
                from sentence_transformers import CrossEncoder
                self.reranker = CrossEncoder('cross-encoder/qnli-distilroberta-base')
            except:
                pass
    
    def retrieve_context(self, query: str, top_k: int = None) -> List[Tuple[str, float]]:
        """æ··åˆæ£€ç´¢ - ç»“åˆå‘é‡æ£€ç´¢å’Œ BM25
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ–‡æ¡£æ•°ï¼ˆé»˜è®¤3ï¼‰
        
        Returns:
            [(æ–‡æœ¬å†…å®¹, ç›¸ä¼¼åº¦å¾—åˆ†), ...]
        """
        top_k = top_k or Config.TOP_K
        
        # 1ï¸âƒ£ å‘é‡æ£€ç´¢
        vector_results = self.vector_store.similarity_search_with_score(
            query=query,
            k=top_k
        )
        
        # 2ï¸âƒ£ BM25 æ£€ç´¢
        bm25_results = self.bm25_retriever.search(
            query=query,
            k=top_k
        )
        
        # 3ï¸âƒ£ ç»“æœèåˆ
        # åˆå¹¶ä¸¤ä¸ªç»“æœé›†ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
        merged = {}
        
        for doc, score in vector_results:
            doc_id = doc.metadata.get('source', doc.page_content[:50])
            merged[doc_id] = {
                'doc': doc,
                'vector_score': score,
                'bm25_score': 0.0
            }
        
        for doc, score in bm25_results:
            doc_id = doc.metadata.get('source', doc.page_content[:50])
            if doc_id not in merged:
                merged[doc_id] = {
                    'doc': doc,
                    'vector_score': 0.0,
                    'bm25_score': score
                }
            else:
                merged[doc_id]['bm25_score'] = score
        
        # è®¡ç®—èåˆå¾—åˆ†
        final_results = [
            (
                item['doc'],
                0.6 * item['vector_score'] + 0.4 * item['bm25_score']
            )
            for item in merged.values()
        ]
        
        # 4ï¸âƒ£ ç²¾æ’ï¼ˆå¯é€‰ï¼‰
        if self.reranker and len(final_results) > 2:
            reranked = self._rerank(query, final_results)
            final_results = reranked
        
        # è¿”å›å‰ K ä¸ªç»“æœ
        return sorted(final_results, key=lambda x: x[1], reverse=True)[:top_k]
    
    def query(
        self,
        query: str,
        conversation_history: str = "",
        stream: bool = True
    ) -> Union[str, Generator[str, None, None]]:
        """æ‰§è¡Œ RAG æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            conversation_history: å¯¹è¯å†å²ï¼ˆæ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        
        Returns:
            å›ç­”ï¼ˆæµæˆ–å­—ç¬¦ä¸²ï¼‰
        """
        
        # 1ï¸âƒ£ æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.retrieve_context(query)
        
        # 2ï¸âƒ£ æ„å»º Prompt
        context = "\n\n".join([
            f"ã€æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}ã€‘\n{doc.page_content}"
            for doc, _ in docs
        ])
        
        prompt = self.DEFAULT_PROMPT_TEMPLATE.format(
            conversation_history=conversation_history,
            context=context,
            question=query
        )
        
        # 3ï¸âƒ£ è°ƒç”¨ LLM
        if stream:
            return self._stream_response(prompt)
        else:
            response = self.llm.invoke(prompt)
            return response.content
    
    def _stream_response(self, prompt: str) -> Generator[str, None, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        
        try:
            # ä½¿ç”¨ LLM çš„æµå¼æ¥å£
            for chunk in self.llm.stream(prompt):
                if isinstance(chunk, str):
                    yield chunk
                elif hasattr(chunk, 'content'):
                    yield chunk.content
        except Exception as e:
            self.logger.error(f"æµå¼ç”Ÿæˆå‡ºé”™: {str(e)}")
            yield f"[é”™è¯¯] {str(e)}"
```

### 3. å‘é‡å­˜å‚¨ç±»

**æ–‡ä»¶**: `src/core/vector_store.py`

```python
class VectorStore:
    """å‘é‡å­˜å‚¨ä¸æ£€ç´¢"""
    
    def __init__(self, persist_directory: str = "./vector_db"):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        self.persist_directory = persist_directory
        self.embeddings = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL
        )
        self.vectorstore = None
        self.load_or_create()
    
    def load_or_create(self):
        """åŠ è½½ç°æœ‰å‘é‡åº“æˆ–åˆ›å»ºæ–°çš„"""
        
        # å°è¯•åŠ è½½ç°æœ‰å‘é‡åº“
        try:
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            self.logger.info(f"âœ“ åŠ è½½ç°æœ‰å‘é‡åº“ï¼Œå…± {self.vectorstore._collection.count()} ä¸ªæ–‡æ¡£")
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½å‘é‡åº“: {e}ï¼Œåˆ›å»ºæ–°å‘é‡åº“")
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
    
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“
        
        Args:
            documents: LangChain Document å¯¹è±¡åˆ—è¡¨
        """
        
        if not documents:
            self.logger.warning("æ²¡æœ‰æ–‡æ¡£è¦æ·»åŠ ")
            return
        
        self.logger.info(f"å¼€å§‹å‘é‡åŒ– {len(documents)} ä¸ªæ–‡æ¡£å—...")
        
        # ä½¿ç”¨ Chroma çš„ add_documents æ–¹æ³•
        # å†…éƒ¨ä¼šè‡ªåŠ¨è¿›è¡Œå‘é‡åŒ–å’Œå­˜å‚¨
        ids = self.vectorstore.add_documents(documents)
        
        # æŒä¹…åŒ–åˆ°ç£ç›˜
        self.vectorstore.persist()
        
        self.logger.info(f"âœ“ æˆåŠŸæ·»åŠ  {len(ids)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡åº“")
    
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 3
    ) -> List[Tuple[Document, float]]:
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ–‡æ¡£æ•°
        
        Returns:
            [(Document, ç›¸ä¼¼åº¦å¾—åˆ†), ...]
            ç›¸ä¼¼åº¦èŒƒå›´ï¼š0-1ï¼Œè¶Šæ¥è¿‘ 1 è¶Šç›¸ä¼¼
        """
        
        if self.vectorstore is None:
            self.logger.warning("å‘é‡åº“ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        try:
            # Chroma ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            return results
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def similarity_search(self, query: str, k: int = 3) -> List[Document]:
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸å¸¦åˆ†æ•°ï¼‰"""
        
        if self.vectorstore is None:
            return []
        
        return self.vectorstore.similarity_search(query, k=k)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯"""
        
        if self.vectorstore is None:
            return {'status': 'empty'}
        
        try:
            count = self.vectorstore._collection.count()
            return {
                'status': 'loaded',
                'document_count': count,
                'embedding_dim': Config.EMBEDDING_DIM,
                'persist_directory': self.persist_directory
            }
        except Exception as e:
            return {'status': 'error', 'error': str(e)}
```

### 4. æ–‡æ¡£å¤„ç†å™¨ç±»

**æ–‡ä»¶**: `src/core/document_processor.py`

```python
class DocumentProcessor:
    """æ–‡æ¡£åŠ è½½å’Œå¤„ç†"""
    
    def __init__(
        self,
        chunk_size: int = 1500,
        chunk_overlap: int = 300
    ):
        """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨"""
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # é€’å½’æ–‡æœ¬åˆ†å‰²å™¨
        # æŒ‰ä¼˜å…ˆçº§å°è¯•åˆ†å‰²ï¼šæ ‡é¢˜ â†’ æ®µè½ â†’ å¥å­ â†’ å­—ç¬¦
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=[
                "\n## ",      # Markdown äºŒçº§æ ‡é¢˜ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                "\n### ",     # Markdown ä¸‰çº§æ ‡é¢˜
                "\n#### ",    # Markdown å››çº§æ ‡é¢˜
                "\n\n",       # æ®µè½åˆ†å‰²
                "\n",         # æ¢è¡Œç¬¦
                "ã€‚",         # ä¸­æ–‡å¥å·
                "ï¼",         # ä¸­æ–‡æ„Ÿå¹å·
                "ï¼Ÿ",         # ä¸­æ–‡é—®å·
                "ï¼›",         # ä¸­æ–‡åˆ†å·
                " "           # ç©ºæ ¼
            ]
        )
    
    def load_document(self, file_path: str) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            Document å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡ä»£è¡¨æ–‡ä»¶çš„ä¸€é¡µ/éƒ¨åˆ†
        """
        
        file_path = Path(file_path)
        extension = file_path.suffix.lower()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
        if extension == ".pdf":
            loader = PyPDFLoader(str(file_path))
        elif extension in [".txt", ".md"]:
            loader = TextLoader(str(file_path), encoding="utf-8")
        elif extension in [".docx", ".doc"]:
            loader = Docx2txtLoader(str(file_path))
        elif extension == ".csv":
            loader = CSVLoader(str(file_path), encoding="utf-8")
        elif extension == ".json":
            loader = JSONLoader(str(file_path), jq_schema=".")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {extension}")
        
        # åŠ è½½æ–‡æ¡£
        docs = loader.load()
        
        # é™„åŠ å…ƒæ•°æ®
        for doc in docs:
            if 'source' not in doc.metadata:
                doc.metadata['source'] = str(file_path.name)
        
        return docs
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """å°†æ–‡æ¡£åˆ†å‰²æˆæ–‡æœ¬å—
        
        Args:
            documents: Document å¯¹è±¡åˆ—è¡¨
        
        Returns:
            åˆ†å‰²åçš„ Document å¯¹è±¡åˆ—è¡¨
        """
        
        # ä½¿ç”¨é€’å½’æ–‡æœ¬åˆ†å‰²å™¨
        chunks = self.splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ ç´¢å¼•
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_index'] = i
        
        return chunks
    
    def process_documents(
        self,
        file_paths: List[str],
        progress_callback=None
    ) -> List[Document]:
        """æ‰¹å¤„ç†å¤šä¸ªæ–‡æ¡£
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total) -> None
        
        Returns:
            æ‰€æœ‰åˆ†å‰²åçš„æ–‡æœ¬å—
        """
        
        all_documents = []
        
        for i, file_path in enumerate(file_paths):
            try:
                # åŠ è½½æ–‡æ¡£
                docs = self.load_document(file_path)
                
                # åˆ†å‰²æ–‡æ¡£
                chunks = self.split_documents(docs)
                
                all_documents.extend(chunks)
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i + 1, len(file_paths))
                
                print(f"âœ“ å¤„ç†äº† {file_path}ï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                print(f"âœ— å¤„ç† {file_path} å¤±è´¥: {str(e)}")
                continue
        
        print(f"âœ“ æ€»å…±å¤„ç†äº† {len(all_documents)} ä¸ªæ–‡æœ¬å—")
        return all_documents
```

---

## ç®—æ³•è¯¦è§£

### 1. æ··åˆæ£€ç´¢ç®—æ³•

**æ··åˆæ£€ç´¢ç»¼åˆäº†å‘é‡æ£€ç´¢å’Œ BM25 æ£€ç´¢çš„ä¼˜ç‚¹ï¼š**

```python
def hybrid_retrieve(query: str, k: int = 3):
    """æ··åˆæ£€ç´¢ç®—æ³•"""
    
    # æ­¥éª¤1ï¼šå‘é‡æ£€ç´¢
    vector_results = vector_store.similarity_search_with_score(query, k=k)
    # è¿”å›æ ¼å¼: [(doc1, 0.95), (doc2, 0.87), ...]
    # åˆ†æ•°èŒƒå›´: [0, 1]ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
    
    # æ­¥éª¤2ï¼šBM25 æ£€ç´¢
    bm25_results = bm25_retriever.search(query, k=k)
    # è¿”å›æ ¼å¼: [(doc1, 2.3), (doc2, 1.8), ...]
    # åˆ†æ•°èŒƒå›´: [0, âˆ)ï¼Œä½¿ç”¨ TF-IDF ç®—æ³•
    
    # æ­¥éª¤3ï¼šæ ‡å‡†åŒ– BM25 åˆ†æ•°åˆ° [0, 1]
    max_bm25_score = max([score for _, score in bm25_results]) if bm25_results else 1
    normalized_bm25 = [
        (doc, score / max_bm25_score)
        for doc, score in bm25_results
    ]
    
    # æ­¥éª¤4ï¼šç»“æœèåˆ
    # åˆ›å»ºæ–‡æ¡£IDåˆ°ä¿¡æ¯çš„æ˜ å°„
    merged = {}
    
    for doc, v_score in vector_results:
        doc_id = id(doc)  # ä½¿ç”¨å¯¹è±¡ ID ä½œä¸ºé”®
        merged[doc_id] = {
            'doc': doc,
            'vector_score': v_score,
            'bm25_score': 0.0
        }
    
    for doc, b_score in normalized_bm25:
        doc_id = id(doc)
        if doc_id not in merged:
            merged[doc_id] = {
                'doc': doc,
                'vector_score': 0.0,
                'bm25_score': b_score
            }
        else:
            merged[doc_id]['bm25_score'] = b_score
    
    # æ­¥éª¤5ï¼šåŠ æƒèåˆ
    # æƒé‡è®¾ç½®ï¼šå‘é‡æ£€ç´¢ 60% + BM25 æ£€ç´¢ 40%
    # ç†ç”±ï¼šå‘é‡æ£€ç´¢æ›´èƒ½ç†è§£è¯­ä¹‰ç›¸å…³æ€§ï¼Œæƒé‡æ›´é«˜
    final_scores = []
    for item in merged.values():
        fused_score = (
            0.6 * item['vector_score'] +
            0.4 * item['bm25_score']
        )
        final_scores.append((item['doc'], fused_score))
    
    # æ­¥éª¤6ï¼šæ’åºå¹¶è¿”å›å‰ K ä¸ª
    final_scores.sort(key=lambda x: x[1], reverse=True)
    return final_scores[:k]
```

**ä¸ºä»€ä¹ˆè¿™æ ·æƒè¡¡ï¼Ÿ**

| æƒé‡åˆ†é… | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|---------|------|------|
| å‘é‡ 70% + BM25 30% | æ›´é‡è§†è¯­ä¹‰ | å¯èƒ½é—æ¼å…³é”®è¯ç²¾ç¡®åŒ¹é… |
| å‘é‡ 60% + BM25 40% | **å¹³è¡¡** | - |
| å‘é‡ 50% + BM25 50% | å¹³è¡¡ | å¯èƒ½è¢«å…³é”®è¯æä¹± |

### 2. CrossEncoder ç²¾æ’ç®—æ³•

**å¦‚æœå¯ç”¨ç²¾æ’ï¼Œä½¿ç”¨æ·±åº¦æ¨¡å‹é‡æ–°è¯„åˆ†**ï¼š

```python
def rerank(query: str, candidates: List[Tuple[Document, float]]):
    """ä½¿ç”¨ CrossEncoder è¿›è¡Œç²¾æ’
    
    CrossEncoder ä¸è®¡ç®—ç›¸ä¼¼åº¦ï¼Œè€Œæ˜¯ç›´æ¥é¢„æµ‹ç›¸å…³æ€§å¾—åˆ†
    """
    
    from sentence_transformers import CrossEncoder
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    model = CrossEncoder('cross-encoder/qnli-distilroberta-base')
    
    # å‡†å¤‡è¯„åˆ†è¾“å…¥
    # æ ¼å¼: [(query, document_text), ...]
    pairs = [
        [query, doc.page_content]
        for doc, _ in candidates
    ]
    
    # æ‰¹é‡è¯„åˆ†
    # è¿”å›: [score1, score2, ...] èŒƒå›´ [0, 1]
    scores = model.predict(pairs)
    
    # æ­¥éª¤4ï¼šé‡æ–°æ’åº
    ranked = [
        (doc, score)
        for (doc, _), score in zip(candidates, scores)
    ]
    
    ranked.sort(key=lambda x: x[1], reverse=True)
    
    return ranked
```

**CrossEncoder vs Bi-Encoderï¼ˆå‘é‡æ£€ç´¢ï¼‰**ï¼š

| æ–¹é¢ | Bi-Encoder | CrossEncoder |
|------|-----------|------------|
| **è®¡ç®—æ–¹å¼** | åˆ†åˆ«ç¼–ç  query å’Œ docï¼Œè®¡ç®—ç›¸ä¼¼åº¦ | åŒæ—¶ç¼–ç  query å’Œ docï¼Œç›´æ¥é¢„æµ‹ç›¸å…³æ€§ |
| **å‡†ç¡®åº¦** | ä¸­ç­‰ | âœ… é«˜ |
| **è®¡ç®—é‡** | ä½ï¼ˆå‘é‡åŒ– 1 æ¬¡ï¼‰ | é«˜ï¼ˆé€å¯¹è®¡ç®—ï¼‰ |
| **é€‚ç”¨** | åˆç­›ï¼ˆå¿«é€Ÿï¼‰ | ç²¾æ’ï¼ˆå‡†ç¡®ï¼‰ |

### 3. ReAct æ¨ç†å¾ªç¯çš„çŠ¶æ€æœº

```
çŠ¶æ€è½¬ç§»å›¾ï¼š

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   IDLE      â”‚ (åˆå§‹åŒ–)
    â”‚  (ç©ºé—²)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ æ”¶åˆ°é—®é¢˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  THINKING   â”‚ æ€è€ƒéœ€è¦åšä»€ä¹ˆ
    â”‚  (æ€è€ƒ)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  æ˜¯å¦éœ€è¦å·¥å…·ï¼Ÿ      â”‚
    â”‚  (åˆ¤æ–­)             â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
    æ˜¯   â”‚            â”‚   å¦
        â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ACTING     â”‚  â”‚  COMPLETED   â”‚
    â”‚  (æ‰§è¡Œå·¥å…·)   â”‚  â”‚  (å®Œæˆ)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â–²
           â–¼                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
    â”‚  REFLECTING  â”‚â”€â”€â”€â”€â”€â”€â”€â”˜ (æœ€ç»ˆç­”æ¡ˆ)
    â”‚  (åæ€ç»“æœ)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ éœ€è¦ç»§ç»­ï¼Ÿ
           â”œâ”€ æ˜¯ â†’ THINKING
           â””â”€ å¦ â†’ COMPLETED
```

### 4. BM25 ç®—æ³•ç»†èŠ‚

**BM25 æ˜¯ä¿¡æ¯æ£€ç´¢ä¸­çš„ç»å…¸ç®—æ³•ï¼Œä½¿ç”¨ TF-IDF å’Œæ¦‚ç‡æ¨¡å‹**ï¼š

```python
def bm25_score(query: str, document: str) -> float:
    """BM25 è¯„åˆ†å…¬å¼"""
    
    # å‚æ•°è®¾ç½®ï¼ˆLangChain é»˜è®¤å€¼ï¼‰
    k1 = 1.5      # è¯é¢‘é¥±å’Œåº¦ï¼ˆè¶Šå°è¶Šå¿«é¥±å’Œï¼‰
    b = 0.75      # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼ˆ0 = å…³é—­ï¼Œ1 = å®Œå…¨ï¼‰
    
    # æ­¥éª¤1ï¼šåˆ†è¯
    query_terms = query.split()  # å®é™…ä½¿ç”¨æ›´å¤æ‚çš„åˆ†è¯å™¨
    doc_terms = document.split()
    
    # æ­¥éª¤2ï¼šè®¡ç®—æ–‡æ¡£é•¿åº¦
    doc_length = len(doc_terms)
    avg_doc_length = 150  # å‡è®¾å¹³å‡æ–‡æ¡£é•¿åº¦ 150
    
    # æ­¥éª¤3ï¼šé€é¡¹è®¡ç®— BM25 åˆ†æ•°
    score = 0.0
    idf_cache = {}  # ç¼“å­˜ IDF å€¼
    
    for term in query_terms:
        # è®¡ç®— IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
        if term not in idf_cache:
            # N = æ€»æ–‡æ¡£æ•°, df = åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°
            N = 10000
            df = count_docs_with_term(term)  # ä»ç´¢å¼•ä¸­è·å–
            idf = log((N - df + 0.5) / (df + 0.5) + 1)
            idf_cache[term] = idf
        else:
            idf = idf_cache[term]
        
        # è®¡ç®—è¯é¢‘ (TF)
        tf = doc_terms.count(term)
        
        # BM25 å…¬å¼
        # score = IDF * (TF * (k1 + 1)) / (TF + k1 * (1 - b + b * doc_length / avg_doc_length))
        numerator = tf * (k1 + 1)
        denominator = tf + k1 * (1 - b + b * doc_length / avg_doc_length)
        
        term_score = idf * (numerator / denominator)
        score += term_score
    
    return score
```

**ä¸ºä»€ä¹ˆ BM25 åœ¨ RAG ä¸­æœ‰ç”¨ï¼Ÿ**

- **ç²¾ç¡®åŒ¹é…**ï¼šå¯¹äºäººåã€æŠ€æœ¯æœ¯è¯­ç­‰å…³é”®è¯æœ‰ä¼˜åŠ¿
- **é€Ÿåº¦å¿«**ï¼šä¸éœ€è¦ç¥ç»ç½‘ç»œï¼ŒåŸºäºç»Ÿè®¡å­¦
- **å¯è§£é‡Š**ï¼šå¯ä»¥çœ‹åˆ°æ¯ä¸ªè¯è´¡çŒ®äº†å¤šå°‘åˆ†æ•°

---

## å¼‚å¸¸å¤„ç†ä¸æ—¥å¿—

### 1. è‡ªå®šä¹‰å¼‚å¸¸ç±»

```python
# src/utils/exceptions.py

class RAGException(Exception):
    """RAG ç³»ç»ŸåŸºå¼‚å¸¸"""
    pass

class VectorStoreException(RAGException):
    """å‘é‡åº“å¼‚å¸¸"""
    pass

class DocumentProcessingException(RAGException):
    """æ–‡æ¡£å¤„ç†å¼‚å¸¸"""
    pass

class LLMException(RAGException):
    """LLM è°ƒç”¨å¼‚å¸¸"""
    pass

class RetrievalException(RAGException):
    """æ£€ç´¢å¼‚å¸¸"""
    pass

class AgentException(RAGException):
    """Agent æ‰§è¡Œå¼‚å¸¸"""
    pass
```

### 2. æ—¥å¿—é…ç½®

```python
# src/utils/logger.py

import logging
from logging.handlers import RotatingFileHandler
from pathlib import Path

def setup_logging(log_dir: str = "./logs"):
    """é…ç½®å…¨å±€æ—¥å¿—ç³»ç»Ÿ"""
    
    log_dir = Path(log_dir)
    log_dir.mkdir(exist_ok=True)
    
    # åˆ›å»º logger
    logger = logging.getLogger("rag_system")
    logger.setLevel(logging.DEBUG)
    
    # æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¸¦è½®è½¬ï¼‰
    file_handler = RotatingFileHandler(
        filename=log_dir / "app.log",
        maxBytes=10 * 1024 * 1024,  # 10 MB
        backupCount=5,               # ä¿ç•™ 5 ä¸ªå¤‡ä»½
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger
```

### 3. å¼‚å¸¸å¤„ç†ç¤ºä¾‹

```python
# src/services/rag_assistant.py

def query(self, question: str) -> str:
    """RAG æŸ¥è¯¢ï¼ŒåŒ…å«å®Œæ•´çš„å¼‚å¸¸å¤„ç†"""
    
    logger = logging.getLogger(__name__)
    
    try:
        # è¾“å…¥éªŒè¯
        if not question or not isinstance(question, str):
            raise ValueError("é—®é¢˜å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
        
        logger.info(f"æ”¶åˆ°æŸ¥è¯¢: {question[:50]}...")
        
        # æ£€ç´¢
        try:
            docs = self.retrieve_context(question)
            if not docs:
                logger.warning("æ£€ç´¢è¿”å›ç©ºç»“æœ")
                return "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        except RetrievalException as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {str(e)}")
            raise
        
        # LLM ç”Ÿæˆ
        try:
            answer = self.llm.invoke(self._build_prompt(docs, question))
            logger.info(f"ç”Ÿæˆå›ç­”æˆåŠŸï¼Œé•¿åº¦: {len(answer)}")
            return answer.content
        except LLMException as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    except ValueError as e:
        logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {str(e)}")
        return f"è¾“å…¥é”™è¯¯: {str(e)}"
    
    except (RetrievalException, LLMException) as e:
        logger.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return f"ç³»ç»Ÿå‡ºé”™: {str(e)}"
    
    except Exception as e:
        logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {str(e)}", exc_info=True)
        return "ç³»ç»Ÿå‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·é‡è¯•"
```

---

## å•å…ƒæµ‹è¯•

### 1. å‘é‡å­˜å‚¨æµ‹è¯•

```python
# tests/unit/test_vector_store.py

import pytest
from src.core.vector_store import VectorStore
from langchain.schema import Document

@pytest.fixture
def vector_store():
    """åˆ›å»ºä¸´æ—¶å‘é‡å­˜å‚¨"""
    store = VectorStore(persist_directory="/tmp/test_vector_db")
    yield store
    # æ¸…ç†
    import shutil
    shutil.rmtree("/tmp/test_vector_db", ignore_errors=True)

def test_add_documents(vector_store):
    """æµ‹è¯•æ·»åŠ æ–‡æ¡£"""
    
    docs = [
        Document(page_content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€", metadata={"source": "test.txt"}),
        Document(page_content="Java æ˜¯ä¼ä¸šçº§å¼€å‘è¯­è¨€", metadata={"source": "test.txt"}),
    ]
    
    vector_store.add_documents(docs)
    
    # éªŒè¯æ–‡æ¡£å·²æ·»åŠ 
    stats = vector_store.get_stats()
    assert stats['document_count'] == 2

def test_similarity_search(vector_store):
    """æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢"""
    
    # æ·»åŠ æµ‹è¯•æ–‡æ¡£
    docs = [
        Document(page_content="çŒ«æ˜¯å®¶å…»å® ç‰©", metadata={"source": "animals.txt"}),
        Document(page_content="ç‹—æ˜¯å¿ è¯šçš„å® ç‰©", metadata={"source": "animals.txt"}),
        Document(page_content="Python ç¼–ç¨‹è¯­è¨€", metadata={"source": "programming.txt"}),
    ]
    vector_store.add_documents(docs)
    
    # æœç´¢ç›¸ä¼¼æ–‡æ¡£
    results = vector_store.similarity_search("å® ç‰©", k=2)
    
    # åº”è¯¥è¿”å›å…³äºå® ç‰©çš„ä¸¤ä¸ªæ–‡æ¡£
    assert len(results) == 2
    assert "å® ç‰©" in results[0].page_content or "å® ç‰©" in results[1].page_content
```

### 2. æ–‡æ¡£å¤„ç†æµ‹è¯•

```python
# tests/unit/test_document_processor.py

import pytest
from src.core.document_processor import DocumentProcessor
from pathlib import Path

@pytest.fixture
def processor():
    return DocumentProcessor(chunk_size=1000, chunk_overlap=100)

@pytest.fixture
def sample_markdown(tmp_path):
    """åˆ›å»ºæ ·æœ¬ Markdown æ–‡ä»¶"""
    content = """# æ ‡é¢˜ä¸€

è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ã€‚

## å°æ ‡é¢˜

è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œæ›´é•¿ä¸€äº›ã€‚è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œæ›´é•¿ä¸€äº›ã€‚
è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œæ›´é•¿ä¸€äº›ã€‚è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œæ›´é•¿ä¸€äº›ã€‚

## å¦ä¸€ä¸ªå°æ ‡é¢˜

æœ€åä¸€æ®µã€‚
"""
    
    file_path = tmp_path / "test.md"
    file_path.write_text(content, encoding='utf-8')
    return str(file_path)

def test_load_document(processor, sample_markdown):
    """æµ‹è¯•åŠ è½½æ–‡æ¡£"""
    
    docs = processor.load_document(sample_markdown)
    
    # Markdown æ–‡ä»¶åº”è¯¥åŠ è½½ä¸ºå•ä¸ªæ–‡æ¡£
    assert len(docs) > 0
    assert "æ ‡é¢˜ä¸€" in docs[0].page_content

def test_split_documents(processor, sample_markdown):
    """æµ‹è¯•åˆ†å‰²æ–‡æ¡£"""
    
    docs = processor.load_document(sample_markdown)
    chunks = processor.split_documents(docs)
    
    # åº”è¯¥è¢«åˆ†å‰²ä¸ºå¤šä¸ªå—
    assert len(chunks) > 1
    
    # æ¯ä¸ªå—åº”è¯¥æœ‰å…ƒæ•°æ®
    for chunk in chunks:
        assert 'source' in chunk.metadata
        assert 'chunk_index' in chunk.metadata
        assert len(chunk.page_content) <= 1000 + 100  # chunk_size + overlap
```

### 3. RAG åŠ©æ‰‹æµ‹è¯•

```python
# tests/unit/test_rag_assistant.py

import pytest
from unittest.mock import Mock, patch
from src.services.rag_assistant import RAGAssistant

@pytest.fixture
def mock_vector_store():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„å‘é‡å­˜å‚¨"""
    mock = Mock()
    mock.similarity_search_with_score.return_value = [
        (Mock(page_content="å‘é‡æ•°æ®åº“æ˜¯...", metadata={'source': 'db.md'}), 0.95),
        (Mock(page_content="ChromaDB æ˜¯...", metadata={'source': 'chroma.md'}), 0.87),
    ]
    return mock

def test_retrieve_context(mock_vector_store):
    """æµ‹è¯•æ£€ç´¢ä¸Šä¸‹æ–‡"""
    
    with patch('src.services.rag_assistant.BM25Retriever') as mock_bm25:
        mock_bm25.return_value.search.return_value = [
            (Mock(page_content="å‘é‡æ•°æ®åº“...", metadata={'source': 'db.md'}), 1.5),
        ]
        
        assistant = RAGAssistant(vector_store=mock_vector_store)
        results = assistant.retrieve_context("ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“", top_k=2)
        
        # åº”è¯¥è¿”å›èåˆåçš„ç»“æœ
        assert len(results) > 0
        assert results[0][1] > 0  # æœ‰åˆ†æ•°
```

---

## éƒ¨ç½²ä¸é…ç½®

### 1. ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶ç¤ºä¾‹

# LLM æä¾›è€…é€‰æ‹©
MODEL_PROVIDER=openai          # æˆ– ollama, deepseek

# OpenAI é…ç½®
OPENAI_API_KEY=sk-xxxxxxxxxx
OPENAI_MODEL=gpt-4

# Ollama é…ç½®
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# DeepSeek é…ç½®
DEEPSEEK_API_KEY=sk-xxxxxxxxxx
DEEPSEEK_API_URL=https://api.deepseek.com

# å‘é‡åŒ–æ¨¡å‹
EMBEDDING_MODEL=all-MiniLM-L6-v2

# RAG é…ç½®
RAG_FAST_MODE=true              # å¯ç”¨å¿«é€Ÿæ¨¡å¼
TOP_K=3                         # è¿”å›æ–‡æ¡£æ•°
CHUNK_SIZE=1500                 # æ–‡æœ¬å—å¤§å°
CHUNK_OVERLAP=300               # å—ä¹‹é—´é‡å 

# æ£€ç´¢é…ç½®
ENABLE_RERANK=false             # æ˜¯å¦å¯ç”¨ç²¾æ’
ENABLE_HYBRID_RETRIEVAL=true    # å¯ç”¨æ··åˆæ£€ç´¢

# è·¯å¾„é…ç½®
DOCUMENTS_PATH=./documents
VECTOR_DB_PATH=./vector_db
CONVERSATIONS_PATH=./conversations
LOGS_PATH=./logs

# æœåŠ¡å™¨é…ç½®
API_HOST=0.0.0.0
API_PORT=8000
FRONTEND_URL=http://localhost:5173
```

### 2. Docker éƒ¨ç½²

```dockerfile
# Dockerfile

FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨åº”ç”¨
CMD ["uvicorn", "src.api.app:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3. Docker Compose é…ç½®

```yaml
# docker-compose.yml

version: '3.8'

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./documents:/app/documents
      - ./vector_db:/app/vector_db
      - ./logs:/app/logs
    environment:
      - MODEL_PROVIDER=openai
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - chromadb

  frontend:
    image: node:18-alpine
    working_dir: /app
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
    command: npm run dev

  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
```

---

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. æ£€ç´¢æ€§èƒ½æµ‹è¯•

```python
# tests/performance/test_retrieval_speed.py

import time
from src.core.vector_store import VectorStore
from src.core.bm25_retriever import BM25Retriever

def benchmark_vector_retrieval(vector_store, queries, iterations=10):
    """åŸºå‡†æµ‹è¯•å‘é‡æ£€ç´¢é€Ÿåº¦"""
    
    times = []
    
    for _ in range(iterations):
        for query in queries:
            start = time.time()
            results = vector_store.similarity_search(query, k=3)
            elapsed = time.time() - start
            times.append(elapsed)
    
    avg_time = sum(times) / len(times)
    print(f"å‘é‡æ£€ç´¢å¹³å‡è€—æ—¶: {avg_time * 1000:.2f} ms")
    print(f"ååé‡: {1 / avg_time:.0f} QPS")
    
    return avg_time

def benchmark_bm25_retrieval(bm25_retriever, queries, iterations=10):
    """åŸºå‡†æµ‹è¯• BM25 æ£€ç´¢é€Ÿåº¦"""
    
    times = []
    
    for _ in range(iterations):
        for query in queries:
            start = time.time()
            results = bm25_retriever.search(query, k=3)
            elapsed = time.time() - start
            times.append(elapsed)
    
    avg_time = sum(times) / len(times)
    print(f"BM25 æ£€ç´¢å¹³å‡è€—æ—¶: {avg_time * 1000:.2f} ms")
    print(f"ååé‡: {1 / avg_time:.0f} QPS")
    
    return avg_time

def benchmark_hybrid_retrieval(vector_store, bm25_retriever, queries, iterations=10):
    """åŸºå‡†æµ‹è¯•æ··åˆæ£€ç´¢é€Ÿåº¦"""
    
    times = []
    
    for _ in range(iterations):
        for query in queries:
            start = time.time()
            v_results = vector_store.similarity_search(query, k=5)
            b_results = bm25_retriever.search(query, k=5)
            # èåˆ...
            elapsed = time.time() - start
            times.append(elapsed)
    
    avg_time = sum(times) / len(times)
    print(f"æ··åˆæ£€ç´¢å¹³å‡è€—æ—¶: {avg_time * 1000:.2f} ms")
    print(f"ååé‡: {1 / avg_time:.0f} QPS")
    
    return avg_time

# æ€§èƒ½åŸºå‡†ç»“æœç¤ºä¾‹ï¼ˆåœ¨æµ‹è¯•ç¯å¢ƒï¼‰
"""
å‘é‡æ£€ç´¢å¹³å‡è€—æ—¶: 150 ms
ååé‡: 6-7 QPS

BM25 æ£€ç´¢å¹³å‡è€—æ—¶: 50 ms
ååé‡: 20 QPS

æ··åˆæ£€ç´¢å¹³å‡è€—æ—¶: 250 ms (100ms å‘é‡ + 50ms BM25 + 100ms èåˆ)
ååé‡: 4 QPS

LLM ç”Ÿæˆå¹³å‡è€—æ—¶: 5-10 ç§’
ç«¯åˆ°ç«¯æ€»è€—æ—¶: 5-10 ç§’ (LLM æ˜¯ä¸»è¦ç“¶é¢ˆ)
"""
```

### 2. å†…å­˜å ç”¨åˆ†æ

```python
# tests/performance/test_memory_usage.py

import psutil
import tracemalloc

def measure_memory_usage():
    """æµ‹é‡å„æ¨¡å—çš„å†…å­˜å ç”¨"""
    
    process = psutil.Process()
    
    # åˆå§‹å†…å­˜
    initial_mem = process.memory_info().rss / 1024 / 1024  # MB
    
    # åŠ è½½å‘é‡å­˜å‚¨
    vector_store = VectorStore()
    after_vector = process.memory_info().rss / 1024 / 1024
    
    # åŠ è½½ Embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings("all-MiniLM-L6-v2")
    after_embeddings = process.memory_info().rss / 1024 / 1024
    
    # åŠ è½½ LLM
    llm = ChatOpenAI(model_name="gpt-4")
    after_llm = process.memory_info().rss / 1024 / 1024
    
    print(f"""
    å†…å­˜å ç”¨åˆ†æ:
    åˆå§‹: {initial_mem:.1f} MB
    åŠ è½½å‘é‡å­˜å‚¨å: {after_vector:.1f} MB (+{after_vector - initial_mem:.1f} MB)
    åŠ è½½ Embedding å: {after_embeddings:.1f} MB (+{after_embeddings - after_vector:.1f} MB)
    åŠ è½½ LLM å: {after_llm:.1f} MB (+{after_llm - after_embeddings:.1f} MB)
    
    æ€»å ç”¨: {after_llm:.1f} MB
    """)

# è¾“å‡ºç¤ºä¾‹
"""
å†…å­˜å ç”¨åˆ†æ:
åˆå§‹: 250 MB
åŠ è½½å‘é‡å­˜å‚¨å: 550 MB (+300 MB) - ChromaDB SQLite å’Œå‘é‡æ•°æ®
åŠ è½½ Embedding å: 800 MB (+250 MB) - Sentence-Transformers æ¨¡å‹
åŠ è½½ LLM å: 850 MB (+50 MB) - API è°ƒç”¨ï¼Œä¸åŠ è½½æœ¬åœ°æ¨¡å‹

æ€»å ç”¨: 850 MB
"""
```

---

## æ€»ç»“

æœ¬è®²è§£æ·±å…¥è®²è¿°äº†ï¼š

1. **ä»£ç ç»„ç»‡**ï¼šæ¸…æ™°çš„åˆ†å±‚æ¶æ„
2. **å…³é”®ç±»**ï¼šBaseAgentã€RAGAssistantã€VectorStore ç­‰
3. **æ ¸å¿ƒç®—æ³•**ï¼šæ··åˆæ£€ç´¢ã€CrossEncoder ç²¾æ’ã€BM25
4. **å¼‚å¸¸å¤„ç†**ï¼šè‡ªå®šä¹‰å¼‚å¸¸ã€æ—¥å¿—ç³»ç»Ÿ
5. **æµ‹è¯•**ï¼šå•å…ƒæµ‹è¯•ç¤ºä¾‹
6. **éƒ¨ç½²**ï¼šDockerã€ç¯å¢ƒé…ç½®
7. **æ€§èƒ½**ï¼šåŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–æŒ‡æ ‡

å…³é”®æ•°å­—ï¼š
- å“åº”æ—¶é—´ï¼š5-10 ç§’ï¼ˆå¤§éƒ¨åˆ†æ—¶é—´åœ¨ LLMï¼‰
- æ£€ç´¢é€Ÿåº¦ï¼š250-300 msï¼ˆæ··åˆæ£€ç´¢ï¼‰
- å†…å­˜å ç”¨ï¼š~800-1000 MB
- QPSï¼š4-5ï¼ˆå— LLM é™åˆ¶ï¼‰

ä¸‹ä¸€æ­¥å¯ä»¥ï¼š
- å¢åŠ ç¼“å­˜å±‚æé€Ÿ
- ä½¿ç”¨ GPU åŠ é€Ÿ Embedding
- å®ç°åˆ†å¸ƒå¼å‘é‡æ£€ç´¢
- ä¼˜åŒ– Prompt ä»¥å‡å°‘ LLM è¾“å‡º
